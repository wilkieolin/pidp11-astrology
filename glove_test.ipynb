{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7618d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f80286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe word vectors from a specified file.\n",
    "\n",
    "    Args:\n",
    "        glove_file_path (str): Path to the GloVe file (e.g., \"glove.6B.50d.txt\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: (\n",
    "            W_embedding_norm (np.ndarray): Normalized word embedding matrix.\n",
    "            word_to_idx_map (dict): Mapping from word to its index in the embedding matrix.\n",
    "            idx_to_word_map (dict): Mapping from index to word.\n",
    "            vector_dim (int): Dimension of the word vectors.\n",
    "        )\n",
    "    Raises:\n",
    "        FileNotFoundError: If the GloVe file is not found.\n",
    "        ValueError: If the GloVe file is empty or malformed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(glove_file_path):\n",
    "        raise FileNotFoundError(f\"GloVe file not found: {glove_file_path}\")\n",
    "\n",
    "    words_list = []\n",
    "    word_to_raw_vector = {} \n",
    "    expected_dim = None\n",
    "    \n",
    "    print(f\"Loading GloVe model from {glove_file_path}...\")\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            try:\n",
    "                vector = np.array([float(val) for val in parts[1:]])\n",
    "                \n",
    "                if expected_dim is None: # First valid vector sets the expected dimension\n",
    "                    expected_dim = len(vector)\n",
    "                    if expected_dim == 0: # Should not happen with valid GloVe files\n",
    "                        print(f\"Warning: Word '{word}' in line {line_num+1} has zero dimension. Skipping file.\")\n",
    "                        raise ValueError(\"Vector dimension is zero.\")\n",
    "                \n",
    "                if len(vector) == expected_dim:\n",
    "                    words_list.append(word)\n",
    "                    word_to_raw_vector[word] = vector\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping line {line_num+1} due to inconsistent vector dimension. Word: '{word}', Dim: {len(vector)}, Expected: {expected_dim}\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Skipping line {line_num+1} due to non-numeric vector component for word '{word}'.\")\n",
    "                continue\n",
    "            except IndexError:\n",
    "                 print(f\"Warning: Skipping line {line_num+1} due to missing vector components for word '{word}'.\")\n",
    "                 continue\n",
    "    \n",
    "    if not words_list:\n",
    "        raise ValueError(\"GloVe file is empty or no valid word vectors could be parsed.\")\n",
    "\n",
    "    vocab_size = len(words_list)\n",
    "    vector_dim = expected_dim \n",
    "    \n",
    "    word_to_idx_map = {word: idx for idx, word in enumerate(words_list)}\n",
    "    idx_to_word_map = {idx: word for idx, word in enumerate(words_list)}\n",
    "    \n",
    "    W_embedding = np.zeros((vocab_size, vector_dim))\n",
    "    for i, word in enumerate(words_list):\n",
    "        W_embedding[i, :] = word_to_raw_vector[word]\n",
    "        \n",
    "    # Normalize W_embedding rows to unit length for cosine similarity\n",
    "    norms = np.linalg.norm(W_embedding, axis=1, keepdims=True)\n",
    "    # Avoid division by zero for zero-norm vectors (e.g. if a word vector was all zeros)\n",
    "    # Such vectors will remain zero vectors after normalization.\n",
    "    safe_norms = np.where(norms == 0, 1e-10, norms) # Use 1e-10 to prevent division by zero\n",
    "    W_embedding_norm = W_embedding / safe_norms\n",
    "    \n",
    "    print(f\"Loaded {vocab_size} word vectors with dimension {vector_dim}.\")\n",
    "    return W_embedding_norm, word_to_idx_map, idx_to_word_map, vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886a343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_words_to_vector(query_vector, n_words, W_embedding_norm, idx_to_word_map):\n",
    "    \"\"\"\n",
    "    Given a query vector and an integer n, returns the n words closest to that vector\n",
    "    based on cosine similarity with words in the provided embedding matrix.\n",
    "\n",
    "    Args:\n",
    "        query_vector (np.ndarray): The input query vector.\n",
    "        n_words (int): The number of closest words to return.\n",
    "        W_embedding_norm (np.ndarray): The normalized GloVe word embedding matrix.\n",
    "                                      Each row must be a normalized word vector.\n",
    "        idx_to_word_map (dict): A dictionary mapping from row index to word.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (word, cosine_similarity_score), \n",
    "              representing the n closest words. Returns an empty list if n_words <= 0\n",
    "              or if the query_vector has zero norm.\n",
    "    \"\"\"\n",
    "    if n_words <= 0:\n",
    "        print(\"Warning: n_words must be positive.\")\n",
    "        return []\n",
    "\n",
    "    # Normalize the query_vector\n",
    "    query_vector_norm_val = np.linalg.norm(query_vector)\n",
    "    if query_vector_norm_val < 1e-9: # Check for effectively zero norm\n",
    "        print(\"Warning: Query vector has near-zero norm. Cannot compute meaningful similarity.\")\n",
    "        return []\n",
    "    normalized_query_vector = query_vector / query_vector_norm_val\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    # W_embedding_norm is (vocab_size, vector_dim)\n",
    "    # normalized_query_vector is (vector_dim,)\n",
    "    # similarities will be (vocab_size,)\n",
    "    similarities = np.dot(W_embedding_norm, normalized_query_vector)\n",
    "\n",
    "    # Get indices of top n_words similarities\n",
    "    # np.argsort sorts in ascending order, so we use -similarities\n",
    "    top_indices = np.argsort(-similarities)[:n_words]\n",
    "\n",
    "    closest_words = []\n",
    "    for i in top_indices:\n",
    "        closest_words.append((idx_to_word_map[i], similarities[i]))\n",
    "        \n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766dadd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W_normalized_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W_normalized_embeddings\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W_normalized_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "W_normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for NLTK resources, placed here for use by select_glove_subset\n",
    "_NLTK_RESOURCES_DOWNLOADED = False\n",
    "def _ensure_nltk_resources():\n",
    "    \"\"\"Checks for NLTK and required resources, attempts download if missing.\"\"\"\n",
    "    global _NLTK_RESOURCES_DOWNLOADED\n",
    "    if _NLTK_RESOURCES_DOWNLOADED:\n",
    "        return True\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "        nltk.data.find('tokenizers/punkt') # For pos_tag with list of words\n",
    "        _NLTK_RESOURCES_DOWNLOADED = True\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Warning: NLTK library not found. POS filtering will be skipped.\")\n",
    "        print(\"Please install it: pip install nltk\")\n",
    "        return False\n",
    "    except LookupError:\n",
    "        print(\"Warning: NLTK resources 'averaged_perceptron_tagger' or 'punkt' not found for POS filtering.\")\n",
    "        print(\"Attempting to download...\")\n",
    "        try:\n",
    "            import nltk # Ensure nltk is imported before download\n",
    "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            _NLTK_RESOURCES_DOWNLOADED = True\n",
    "            print(\"NLTK resources downloaded successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download NLTK resources: {e}. POS filtering will be skipped.\")\n",
    "            print(\"You may need to download them manually: import nltk; nltk.download('averaged_perceptron_tagger'); nltk.download('punkt')\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_glove_subset(W_embedding_norm_full, idx_to_word_map_full, n1, n2, pos_filter=None):\n",
    "    \"\"\"\n",
    "    Down-selects words from GloVe embeddings by frequency rank and optionally by POS.\n",
    "\n",
    "    Args:\n",
    "        W_embedding_norm_full (np.ndarray): Full normalized GloVe embedding matrix.\n",
    "        idx_to_word_map_full (dict): Full mapping from index to word.\n",
    "        n1 (int): Starting rank (0-indexed, inclusive).\n",
    "        n2 (int): Ending rank (0-indexed, exclusive).\n",
    "        pos_filter (str, optional): None, \"noun\", or \"verb\". Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (\n",
    "            W_subset (np.ndarray): Embeddings for the selected subset.\n",
    "            subset_word_to_idx_map (dict): Word to new index map for the subset.\n",
    "            subset_idx_to_word_map (dict): New index to word map for the subset.\n",
    "        ) or (None, None, None) if inputs are invalid or selection is empty.\n",
    "    \"\"\"\n",
    "    vocab_size_full = W_embedding_norm_full.shape[0]\n",
    "    actual_n1 = max(0, n1)\n",
    "    actual_n2 = min(vocab_size_full, n2)\n",
    "\n",
    "    if actual_n1 >= actual_n2:\n",
    "        print(f\"Warning: Invalid rank range n1={n1}, n2={n2}. Results in empty selection.\")\n",
    "        return np.array([]).reshape(0, W_embedding_norm_full.shape[1]), {}, {}\n",
    "\n",
    "    candidate_indices_full = list(range(actual_n1, actual_n2))\n",
    "    candidate_words = [idx_to_word_map_full[i] for i in candidate_indices_full]\n",
    "    candidate_vectors = W_embedding_norm_full[candidate_indices_full, :]\n",
    "\n",
    "    final_selected_words = candidate_words\n",
    "    W_subset = candidate_vectors\n",
    "\n",
    "    if pos_filter and candidate_words: # Only attempt POS if filter requested and candidates exist\n",
    "        if pos_filter not in [\"noun\", \"verb\"]:\n",
    "            print(f\"Warning: Invalid pos_filter '{pos_filter}'. Must be 'noun' or 'verb'. Skipping POS filter.\")\n",
    "        elif _ensure_nltk_resources():\n",
    "            import nltk # Import here after resource check\n",
    "            tagged_words = nltk.pos_tag(candidate_words)\n",
    "            \n",
    "            filtered_word_vector_pairs = []\n",
    "            for i, (word, tag) in enumerate(tagged_words):\n",
    "                if (pos_filter == \"noun\" and tag.startswith(\"NN\")) or \\\n",
    "                   (pos_filter == \"verb\" and tag.startswith(\"VB\")):\n",
    "                    filtered_word_vector_pairs.append((word, candidate_vectors[i, :]))\n",
    "            \n",
    "            if filtered_word_vector_pairs:\n",
    "                final_selected_words = [pair[0] for pair in filtered_word_vector_pairs]\n",
    "                W_subset = np.array([pair[1] for pair in filtered_word_vector_pairs])\n",
    "            else: # POS filter resulted in no words\n",
    "                final_selected_words = []\n",
    "                W_subset = np.array([]).reshape(0, W_embedding_norm_full.shape[1])\n",
    "                print(f\"Warning: POS filter '{pos_filter}' resulted in 0 words from the rank selection.\")\n",
    "\n",
    "    subset_word_to_idx_map = {word: i for i, word in enumerate(final_selected_words)}\n",
    "    subset_idx_to_word_map = {i: word for i, word in enumerate(final_selected_words)}\n",
    "\n",
    "    print(f\"Selected {len(final_selected_words)} words from ranks {actual_n1}-{actual_n2-1}\" +\n",
    "          (f\" with POS filter '{pos_filter}'.\" if pos_filter and _NLTK_RESOURCES_DOWNLOADED else \".\"))\n",
    "    return W_subset, subset_word_to_idx_map, subset_idx_to_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebedce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model from glove.6B/glove.6B.50d.txt...\n",
      "Loaded 400000 word vectors with dimension 50.\n",
      "\n",
      "--- Example 1: Closest words to a purely random vector ---\n",
      "Finding 10 closest words to a random vector (dim=50)...\n",
      "\n",
      "Top 10 closest words to the random vector:\n",
      "- mesnel: 0.5981\n",
      "- kitchenette: 0.5100\n",
      "- kareena: 0.5094\n",
      "- treats: 0.4995\n",
      "- ljubijankic: 0.4976\n",
      "- meals: 0.4951\n",
      "- birthweight: 0.4896\n",
      "- anoushka: 0.4886\n",
      "- imron: 0.4868\n",
      "- tasty: 0.4866\n",
      "\n",
      "--- Example 2: Closest words to the average of 'king' and 'royal' ---\n",
      "Finding 5 closest words to the average vector of ['king', 'royal']...\n",
      "\n",
      "Top 5 closest words to the average of ['king', 'royal']:\n",
      "- king: 0.9055\n",
      "- royal: 0.9055\n",
      "- queen: 0.8439\n",
      "- prince: 0.8210\n",
      "- imperial: 0.8151\n",
      "\n",
      "--- Example 3: Select subset of GloVe words by rank and POS ---\n",
      "Warning: NLTK resources 'averaged_perceptron_tagger' or 'punkt' not found for POS filtering.\n",
      "Attempting to download...\n",
      "NLTK resources downloaded successfully.\n",
      "Selected 47 words from ranks 1000-1099 with POS filter 'noun'.\n",
      "Successfully created a subset of 47 nouns.\n",
      "\n",
      "Top 3 closest words in the noun subset to a random vector:\n",
      "- heart: 0.1608\n",
      "- reach: 0.0994\n",
      "- sources: 0.0931\n"
     ]
    }
   ],
   "source": [
    "# Path to your downloaded GloVe file\n",
    "GLOVE_FILE_PATH = \"glove.6B/glove.6B.50d.txt\" \n",
    "# Ensure this file exists at the specified path or change the path.\n",
    "# You can download GloVe vectors from: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "try:\n",
    "    # Load the GloVe model (this might take a few seconds to a minute)\n",
    "    W_normalized_embeddings, word_to_index, index_to_word, embedding_dim = load_glove_model(GLOVE_FILE_PATH)\n",
    "    \n",
    "    # --- Example 1: Find closest words to a purely random vector ---\n",
    "    print(\"\\n--- Example 1: Closest words to a purely random vector ---\")\n",
    "    # Generate a random vector with the same dimension as the GloVe embeddings\n",
    "    random_vec = np.random.rand(embedding_dim) \n",
    "    num_results = 10\n",
    "    \n",
    "    print(f\"Finding {num_results} closest words to a random vector (dim={embedding_dim})...\")\n",
    "    closest_to_random = find_closest_words_to_vector(random_vec, num_results, W_normalized_embeddings, index_to_word)\n",
    "    \n",
    "    if closest_to_random:\n",
    "        print(f\"\\nTop {num_results} closest words to the random vector:\")\n",
    "        for word, score in closest_to_random:\n",
    "            print(f\"- {word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(\"Could not find closest words for the random vector.\")\n",
    "\n",
    "    # --- Example 2: Find closest words to an average of known words ---\n",
    "    # This demonstrates using a more structured, though still arbitrary, vector.\n",
    "    print(\"\\n--- Example 2: Closest words to the average of 'king' and 'royal' ---\")\n",
    "    target_words = ['king', 'royal']\n",
    "    \n",
    "    if all(w in word_to_index for w in target_words):\n",
    "        word_vectors = [W_normalized_embeddings[word_to_index[w]] for w in target_words]\n",
    "        average_vector = np.mean(word_vectors, axis=0)\n",
    "        \n",
    "        num_results_avg = 5\n",
    "        print(f\"Finding {num_results_avg} closest words to the average vector of {target_words}...\")\n",
    "        closest_to_average = find_closest_words_to_vector(average_vector, num_results_avg, W_normalized_embeddings, index_to_word)\n",
    "\n",
    "        if closest_to_average:\n",
    "            print(f\"\\nTop {num_results_avg} closest words to the average of {target_words}:\")\n",
    "            for word, score in closest_to_average:\n",
    "                # Optionally, filter out the input words themselves if they appear high\n",
    "                if word not in target_words or score < 0.999: # score check for exact match\n",
    "                        print(f\"- {word}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"Could not find closest words for the average of {target_words}.\")\n",
    "    else:\n",
    "        print(f\"One or more words from {target_words} not found in vocabulary for Example 2.\")\n",
    "\n",
    "    # --- Example 3: Using the new select_glove_subset function ---\n",
    "    print(\"\\n--- Example 3: Select subset of GloVe words by rank and POS ---\")\n",
    "    # Select words ranked between 1000 and 1100 (exclusive of 1100), and filter for nouns\n",
    "    rank_n1, rank_n2 = 1000, 1100 # 0-indexed ranks\n",
    "    \n",
    "    W_noun_subset, noun_subset_to_idx, idx_to_noun_subset = select_glove_subset(\n",
    "        W_normalized_embeddings, index_to_word, rank_n1, rank_n2, pos_filter=\"noun\"\n",
    "    )\n",
    "\n",
    "    if W_noun_subset.shape[0] > 0:\n",
    "        print(f\"Successfully created a subset of {W_noun_subset.shape[0]} nouns.\")\n",
    "        # Now you can use this subset with find_closest_words_to_vector\n",
    "        # Note: the query vector should still match the original embedding dimension\n",
    "        if embedding_dim > 0: # Ensure embedding_dim is valid\n",
    "            random_vec_for_subset = np.random.rand(embedding_dim)\n",
    "            num_results_subset = 3\n",
    "            closest_in_noun_subset = find_closest_words_to_vector(\n",
    "                random_vec_for_subset, num_results_subset, W_noun_subset, idx_to_noun_subset\n",
    "            )\n",
    "            print(f\"\\nTop {num_results_subset} closest words in the noun subset to a random vector:\")\n",
    "            for word, score in closest_in_noun_subset:\n",
    "                print(f\"- {word}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"The selection of nouns from ranks {rank_n1}-{rank_n2-1} resulted in an empty set.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nERROR: {e}\")\n",
    "    print(\"Please ensure the GloVe file path is correct and the file exists.\")\n",
    "    print(f\"Expected at: {os.path.abspath(GLOVE_FILE_PATH)}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nERROR during GloVe loading or processing: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical_ndim(cartesian_coords):\n",
    "    \"\"\"\n",
    "    Converts n-dimensional Cartesian coordinates to n-dimensional spherical coordinates.\n",
    "\n",
    "    The spherical coordinates are (r, phi_1, phi_2, ..., phi_{n-1}), where:\n",
    "    - r is the radial distance.\n",
    "    - phi_1, ..., phi_{n-2} are inclination angles in [0, pi].\n",
    "    - phi_{n-1} is the azimuthal angle in (-pi, pi].\n",
    "\n",
    "    The transformation maps (x_0, x_1, ..., x_{n-1}) to (r, phi_1, ..., phi_{n-1}) such that:\n",
    "    x_0 = r * cos(phi_1)\n",
    "    x_1 = r * sin(phi_1) * cos(phi_2)\n",
    "    ...\n",
    "    x_{n-2} = r * sin(phi_1) * ... * sin(phi_{n-2}) * cos(phi_{n-1})\n",
    "    x_{n-1} = r * sin(phi_1) * ... * sin(phi_{n-2}) * sin(phi_{n-1})\n",
    "\n",
    "    Args:\n",
    "        cartesian_coords (array-like): A 1D array or list of n Cartesian coordinates\n",
    "                                       (x_0, x_1, ..., x_{n-1}).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D numpy array of n spherical coordinates\n",
    "                    (r, phi_1, ..., phi_{n-1}).\n",
    "                    Returns an empty array if input is empty.\n",
    "                    If input is 1D, returns [|x_0|].\n",
    "                    If input is the origin, angles are set to 0.\n",
    "    \"\"\"\n",
    "    coords = np.asarray(cartesian_coords, dtype=float)\n",
    "    n = coords.shape[0]\n",
    "\n",
    "    if n == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Calculate radial distance r\n",
    "    r = np.linalg.norm(coords)\n",
    "    \n",
    "    spherical_coords = np.zeros(n)\n",
    "    spherical_coords[0] = r\n",
    "\n",
    "    if n == 1:\n",
    "        return spherical_coords # Only r, which is |x_0|\n",
    "\n",
    "    if r < 1e-12: # Effectively at the origin\n",
    "        # All angles are conventionally 0.\n",
    "        # spherical_coords[1:] are already 0.0 by np.zeros initialization.\n",
    "        return spherical_coords\n",
    "\n",
    "    # Calculate n-1 angles\n",
    "    # spherical_coords[1] is phi_1, ..., spherical_coords[n-1] is phi_{n-1}\n",
    "    \n",
    "    # This variable will hold the sum of squares of remaining coordinates:\n",
    "    # For phi_1 (spherical_coords[1]), sum_sq = x_0^2 + ... + x_{n-1}^2\n",
    "    # For phi_2 (spherical_coords[2]), sum_sq = x_1^2 + ... + x_{n-1}^2\n",
    "    # etc.\n",
    "    current_sum_sq = r**2 \n",
    "    epsilon = 1e-12 # Small number for safe division\n",
    "\n",
    "    for i in range(n - 1): # Loop to calculate n-1 angles\n",
    "        angle_idx_in_spherical = i + 1 # phi_1 is at index 1, phi_2 at index 2, ...\n",
    "\n",
    "        if i < n - 2: \n",
    "            # This is for angles phi_1 to phi_{n-2}\n",
    "            # (spherical_coords[1] to spherical_coords[n-2])\n",
    "            # These are arccos based.\n",
    "            # Angle phi_{i+1} (spherical_coords[i+1]) uses cartesian_coords[i]\n",
    "            \n",
    "            cartesian_component = coords[i]\n",
    "            \n",
    "            denominator = np.sqrt(current_sum_sq)\n",
    "\n",
    "            if denominator < epsilon:\n",
    "                # This implies coords[i], coords[i+1], ..., coords[n-1] are all zero.\n",
    "                # The angle is conventionally 0.\n",
    "                spherical_coords[angle_idx_in_spherical] = 0.0\n",
    "            else:\n",
    "                # Clip ratio for numerical stability with arccos\n",
    "                ratio = np.clip(cartesian_component / denominator, -1.0, 1.0)\n",
    "                spherical_coords[angle_idx_in_spherical] = np.arccos(ratio)\n",
    "            \n",
    "            current_sum_sq -= cartesian_component**2\n",
    "            # Ensure current_sum_sq does not become negative due to floating point errors\n",
    "            current_sum_sq = max(0, current_sum_sq) \n",
    "        else: \n",
    "            # This is for the last angle, phi_{n-1} (spherical_coords[n-1])\n",
    "            # This angle is arctan2 based.\n",
    "            # It uses cartesian_coords[n-2] (as x-like) and cartesian_coords[n-1] (as y-like).\n",
    "            # At this point, i = n-2.\n",
    "            # The angle is spherical_coords[n-1].\n",
    "            \n",
    "            # np.arctan2(y, x)\n",
    "            # y component is coords[n-1]\n",
    "            # x component is coords[n-2]\n",
    "            # If both coords[n-2] and coords[n-1] are zero, arctan2(0,0) is 0, which is correct.\n",
    "            spherical_coords[angle_idx_in_spherical] = np.arctan2(coords[n-1], coords[n-2])\n",
    "            \n",
    "    return spherical_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_spherical_embeddings(glove_file_path,\n",
    "                                          nouns_output_file,\n",
    "                                          verbs_output_file,\n",
    "                                          start_rank=1000,\n",
    "                                          target_count=8192):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings, selects nouns and verbs, converts their embeddings\n",
    "    to spherical coordinates (excluding radius), and saves them to text files.\n",     "    Only words longer than 2 characters are included.\n",
    "    For verbs, an additional filter is applied to reject words ending in '-ing',\n",
    "    aiming to select more infinitive-like forms.\n",     "    \"\"\"\n",
    "    print(\"Starting processing and saving spherical embeddings...\")\n",
    "\n",
    "    # 1. Load full GloVe model\n",
    "    W_full, _, idx_to_word_full, embedding_dim = load_glove_model(glove_file_path)\n",
    "    vocab_size_full = W_full.shape[0]\n",
    "\n",
    "    if embedding_dim == 0:\n",
    "        print(\"Error: Embedding dimension is 0. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    pos_types_to_process = [(\"noun\", \"NN\", nouns_output_file),\n",
    "                              (\"verb\", \"VB\", verbs_output_file)]\n",
    "\n",
    "    for pos_name, pos_tag_prefix, output_file_path in pos_types_to_process:\n",
    "        print(f\"\\nProcessing {pos_name}s...\")\n",
    "        # 2. Select subset by rank and POS\n",
    "        # We select from start_rank to the end of vocab, then take the first target_count.\n",
    "        W_subset, _, subset_idx_to_word = select_glove_subset(\n",
    "            W_full, idx_to_word_full, start_rank, vocab_size_full, pos_filter=pos_name\n",
    "        )\n",
    "\n",
    "        num_candidates_after_pos_filter = W_subset.shape[0]\n",
    "\n",
    "        if num_candidates_after_pos_filter == 0:\n",
    "            print(f\"No {pos_name}s found after POS filtering from rank {start_rank}. Skipping file generation for {output_file_path}\")\n",
    "            continue\n",
    "\n",
    "        criteria_desc = \"longer than 2 characters\"\n",
    "        if pos_name == \"verb\":\n",
    "            criteria_desc += \" and not ending in '-ing' (heuristic for infinitive form)\"\n",
    "\n",
    "        print(f\"Found {num_candidates_after_pos_filter} {pos_name}s after POS filtering. \"\n",     "              f\"Will process up to {target_count} words meeting criteria ({criteria_desc}), excluding radius.\")\n",
    "        \n",
    "        words_written_count = 0\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            for i in range(num_candidates_after_pos_filter):\n",
    "                if words_written_count >= target_count:\n",
    "                    break \n",
    "\n",
    "                word = subset_idx_to_word[i]\n",
    "\n",
    "                # Add check for word length\n",
    "                if len(word) > 2:\n",
    "                    # For verbs, reject words ending with \"-ing\" as a heuristic for non-infinitive forms.\n",
    "                    if pos_name == \"verb\" and word.endswith(\"ing\"):\n",
    "                        # Note: This simple heuristic might also exclude some base form verbs that naturally end in \"ing\" (e.g., \"sing\", \"bring\").\n",
    "                        # However, it primarily targets gerunds/present participles (e.g., \"running\", \"singing\").\n",
    "                        continue # Skip this word\n",
    "\n",
    "                    cartesian_vector = W_subset[i, :]\n",
    "                    spherical_vector = cartesian_to_spherical_ndim(cartesian_vector)\n",
    "\n",
    "                    # spherical_vector[0] is radius, spherical_vector[1:] are angles\n",
    "                    if len(spherical_vector) > 1: # Ensure there are angles to write\n",
    "                        f.write(word)\n",
    "                        for component in spherical_vector[1:]: # Skip radius (the first component)\n",
    "                            f.write(f\" {component:.8f}\") \n",
    "                        f.write(\"\\n\")\n",
    "                        words_written_count += 1\n",
    "        \n",
    "        if words_written_count > 0:\n",
    "            saved_criteria_desc = \"longer than 2 chars\"\n",
    "            if pos_name == \"verb\":\n",
    "                saved_criteria_desc += \", non '-ing' form\"\n",
    "            print(f\"Saved {words_written_count} {pos_name}s ({saved_criteria_desc}, excluding radius) to {output_file_path}\")\n",
    "            \n",
    "            if words_written_count < target_count and num_candidates_after_pos_filter >= target_count:\n",
    "                note_filter_desc = \"length > 2 chars criterion\"\n",
    "                if pos_name == \"verb\":\n",
    "                    note_filter_desc += \" and non '-ing' verb form criterion\"\n",
    "                print(f\"  (Note: Target was {target_count}, but only {words_written_count} words met the filtering criteria ({note_filter_desc}) \"\n",
    "                      f\"from the {num_candidates_after_pos_filter} available candidates after POS filtering.)\")\n",
    "            elif num_candidates_after_pos_filter < target_count and words_written_count < num_candidates_after_pos_filter:\n",
    "                note_filter_desc = \"length and (if applicable) verb form filtering\"\n",
    "                print(f\"  (Note: Initial POS selection had {num_candidates_after_pos_filter} words. \"\n",
    "                      f\"After {note_filter_desc}, {words_written_count} words were saved.)\")\n",
    "        else: # words_written_count == 0\n",
    "            if num_candidates_after_pos_filter > 0:\n",
    "                filter_details = \"longer than 2 characters\"\n",
    "                if pos_name == \"verb\":\n",
    "                    filter_details += \" and not ending in '-ing'\"\n",
    "                print(f\"No {pos_name}s meeting criteria ({filter_details}) found for processing from the selection of {num_candidates_after_pos_filter} words. \"\n",
    "                      f\"File {output_file_path} is empty or not created with content.\")\n",
    "\n",
    "    print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c3f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
